{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c23c71",
   "metadata": {},
   "source": [
    "## Chroma 벡터 DB 사용\n",
    "#### 사전 조건: 사용자는 이 노트북을 사용하기 전에 Chroma에 관련 임베딩이 있도록 \"문서 임베딩으로 Chroma 벡터 DB 채우기\" 작업을 실행해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717c30b",
   "metadata": {},
   "source": [
    "#### 2.5 Chroma Vector DB 새로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b331a3ae-f63b-47c1-bfae-6e1efc5062b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Initialize a connection to the running Chroma DB server\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "## Use the following line to connect from within CML\n",
    "chroma_client = chromadb.PersistentClient(path=\"/home/cdsw/chroma-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98c00c",
   "metadata": {},
   "source": [
    "#### 2.6 Chroma Vector DB 컬렉션 및 컬렉션 객체 수 가져오기\n",
    "이 코드는 임베딩을 관리하고 쿼리하기 위한 데이터베이스인 Chroma DB에 대한 연결을 초기화합니다. 사용할 임베딩 모델을 정의하고, 컬렉션 이름을 'cml-default'로 지정하고, 지정된 임베딩 함수로 해당 컬렉션을 가져오거나 생성하려고 시도합니다. 마지막으로 Chroma DB 인덱스의 총 임베딩 수를 검색하여 인쇄하고, 컬렉션에 대한 통계를 제공합니다.\n",
    "\n",
    "※ GPU를 사용하는 실습이 아니므로 GPU에 관한 경고는 무시하셔도 좋습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b448f4-877d-4854-9c17-9247441b5b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 18:01:13.029971: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-17 18:01:13.030011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-17 18:01:13.036584: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 18:01:13.082096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-17 18:01:19.200704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising Chroma DB connection...\n",
      "Getting 'cml-default' as object...\n",
      "Success\n",
      "Total number of embeddings in Chroma DB index is 27\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL_REPO = os.environ.get(\"HF_EMBEDDING_MODEL_REPO\")\n",
    "EMBEDDING_MODEL_NAME = EMBEDDING_MODEL_REPO.split('/')[-1]\n",
    "EMBEDDING_FUNCTION = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "COLLECTION_NAME = 'cml-default'\n",
    "\n",
    "print(\"initialising Chroma DB connection...\")\n",
    "\n",
    "print(f\"Getting '{COLLECTION_NAME}' as object...\")\n",
    "try:\n",
    "    chroma_client.get_collection(name=COLLECTION_NAME, embedding_function=EMBEDDING_FUNCTION)\n",
    "    print(\"Success\")\n",
    "    collection = chroma_client.get_collection(name=COLLECTION_NAME, embedding_function=EMBEDDING_FUNCTION)\n",
    "except:\n",
    "    print(\"Creating new collection...\")\n",
    "    collection = chroma_client.create_collection(name=COLLECTION_NAME, embedding_function=EMBEDDING_FUNCTION)\n",
    "    print(\"Success\")\n",
    "\n",
    "# Get latest statistics from index\n",
    "current_collection_stats = collection.count()\n",
    "print('Total number of embeddings in Chroma DB index is ' + str(current_collection_stats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b86cf4",
   "metadata": {},
   "source": [
    "#### 2.7 여러 속성을 사용하여 Chroma에 벡터를 채우는 샘플 데모\n",
    "\n",
    "여기서 우리는 지정된 텍스트 콘텐츠, 분류 및 파일 경로를 사용하여 의미 검색을 위해 연관된 메타데이터와 고유 ID가 있는 샘플 문서를 Chroma 벡터 데이터베이스 컬렉션에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0835b3-87e3-4d67-abe2-6c6538f8f488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-azure/topics/ml-requirements-azure.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-azure/topics/ml-limitations-azure.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-azure/topics/ml-azure-planning.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/models/topics/ml-challenges-in-prod.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/user-accounts/topics/ml-user-roles.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-aws/topics/ml-requirements-aws.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-aws/topics/ml-aws-network-planning.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/requirements-aws/topics/ml-limitations-aws.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/projects/topics/ml-editors-configuration-modes.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/api/topics/ml-api-v2.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/ai-inference/topics/ml-caii-key-applications.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/ai-inference/topics/ml-caii-key-features.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/ai-inference/topics/ml-caii-use-caii.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/ai-inference/topics/ml-caii-supported-model-artifact-formats.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/product/topics/ml-product-overview.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/runtimes-preinstalled-packages/topics/ml-runtimes-pkgs-2024-05-2-python-3-11-jupyterlab.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/runtimes-preinstalled-packages/topics/ml-runtimes-pkgs-2024-05-2-python-3-10-jupyterlab.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-spark-on-kubernetes.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-cml.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview1.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-autoscaling-overview.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-critical-and-noncritical-pods.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-runtimes.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-provisioning.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/release-notes/topics/ml-whats-new.txt\n",
      "Add of existing embedding ID: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/release-notes/topics/ml-dl-compatibility.txt\n",
      "Add of existing embedding ID: /example/of/file/path/to/doc.txt\n",
      "Insert of existing embedding ID: /example/of/file/path/to/doc.txt\n"
     ]
    }
   ],
   "source": [
    "## Sample add to Chroma vector DB\n",
    "file_path = '/example/of/file/path/to/doc.txt'\n",
    "classification = \"public\"\n",
    "text = \"This is a sample document which would represent content for a semantic search.\"\n",
    "\n",
    "collection.add(\n",
    "    documents=[text],\n",
    "    metadatas=[{\"classification\": classification}],\n",
    "    ids=[file_path]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193156d",
   "metadata": {},
   "source": [
    "#### 2.8 Chroma에서 벡터 쿼리를 수행하고 메타데이터를 사용하여 노이즈를 줄이는 샘플 데모\n",
    "\n",
    "이 코드는 샘플 쿼리 텍스트를 사용하여 Chroma 벡터 데이터베이스에서 의미 검색을 수행하고 가장 유사한 두 결과를 검색합니다. 메타데이터는 메타데이터 필드를 기반으로 필터를 지정하여 검색 결과를 더욱 세분화하는 데 사용할 수 있으므로 보다 정확하고 컨텍스트를 인식하는 쿼리가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5448a26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['/home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-cml.txt', '/home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-provisioning.txt']], 'distances': [[1.501272571600087, 1.5616063231252637]], 'metadatas': [[{'classification': 'public'}, {'classification': 'public'}]], 'embeddings': None, 'documents': [[\"Cloudera AI ArchitectureCloudera Docs\\nCloudera AI Architecture\\nOnce a Cloudera AI Workbench is provisioned, you can start using Cloudera AI for your end-to-end Machine Learning workflow. \\nCloudera AI is a three-tier application that consists of a presentation tier, an application tier\\nand a data tier. \\nWeb tier\\nCloudera AI is a web application that provides a UI that simplifies the action of managing\\nworkloads and resources for data scientists. It offers users a convenient way to deploy\\nand scale their analytical pipeline and collaborate with their colleagues in a secure\\ncompartmentalized environment. \\nCloudera AI communicates using HTTPS, Websocket, and gRPC. External communication is limited to\\nHTTP and Websocket for the web UI and APIs. In-cluster service-to-service communication\\nuses gRPC and is encrypted and mutually authenticated using TLS (Transport Layer\\nSecurity). \\n\\nApplication tier\\nThe application tier uses an actual set of workloads that users are running. These\\nworkloads are isolated in Kubernetes namespaces and run on specially marked compute\\nnodes. Kubernetes/node level auto scaling is used to expand/contract the cluster size\\nbased on user demand.\\nUser code gets baked into Docker images via a set of Source-to-Image pods (S2I), which\\nincludes a managing server, a queue for builds, a registry that serves the images for\\nDocker, a git server, and the builders that actually perform the image building.\\nTraditionally these images used the host machine's docker, but Cloudera AI switched to\\nin-container builds for security and compliance on some platforms. \\n\\nData tier\\nCloudera AI uses an internal Postgres database for persisting metadata of user workloads such as\\nSessions, Jobs, Models and Applications, which runs as a pod and is backed by a\\npersistent volume, using the cloud-provider's block storage offering (for example, EBS\\nfor AWS and Premium_LRS for Azure). \\nCloudera AI uses an NFS server, i.e. a POSIX-compliant file system, for storing users project\\nfiles, which include user code, libraries installed into a Session, and small data\\nfiles. For AWS, Cloudera AI creates an Elastic File System (EFS) file system when provisioning\\nthe workbench for storing project files. For Azure, users can either provide an NFS\\nvolume created in Azure NetApp Files or an external NFS server when provisioning Cloudera AI Workbench for storing project files. This NFS server is supplied to the Cloudera AI Workbenches\\nas Kubernetes Persistent Volumes (PVs). Persistent Volume Claims (PVCs) are set up\\nper-namespace, and each user gets their own namespace - thus each users view of the NFS\\nserver is limited to that exposed by the Cloudera Private Cloud. \\n\\n\\nParent topic: Architecture Overview\", 'ProvisioningCloudera Docs\\nProvisioning\\nCloudera AI utilizes the Cloudera Control Plane to manage Data Services so you can provision and\\ndelete Cloudera AI Workbench. Cloudera Control Plane leverages cloud native capabilities to dynamically\\naccess CPU, memory, and GPU resources along with cloud-managed Kubernetes (K8s) to provide the\\ninfrastructure. \\nDuring provisioning the Cloudera AI application is configured to authenticate end\\nusers of the service (Cloudera AI) via the Cloudera identity provider, which is chained back to the\\ncustomers identity provider. As a result, Cloudera AI provisioned instances allow for\\nseamless customer SSO. \\nWhen you provision a Cloudera AI Workbench, the following happens:\\n\\nCloudera Control Plane performs the following in the cloud environment: \\nRequests a TLS Certificate and domain name with the cloudera.site domain\\nIdentifies the SSO configuration\\nIdentifies the SDX configuration for the environment \\n\\n Cloudera Control Plane provisions a managed Kubernetes cluster \\nCloudera Control Plane installs Cloudera AI into the Kubernetes environment \\nStorage is mounted directly via managed service providers \\n\\nCloudera AI uses the cloud provider load balancer and networking infrastructure to partition the\\nresources. Cloudera AI also leverages the cloud provider infrastructure to enable the customer to\\nspecify autoscaling. \\nCloudera AI provisions the DNS and the certificate. Cloudera AI renews the certificates for the customer on\\nan ongoing basis. \\n\\nParent topic: Architecture Overview']]}\n"
     ]
    }
   ],
   "source": [
    "## Query Chroma vector DB \n",
    "## This query returns the two most similar results from a semantic search\n",
    "results = collection.query(\n",
    "    query_texts=[\"What is Apache Iceberg?\"],\n",
    "    n_results=2\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ac300",
   "metadata": {},
   "source": [
    "#### 2.9 로컬 파일 시스템의 원본 파일(전체 파일)에 매핑하기 위해 Chroma를 사용한 결과\n",
    "\n",
    "이 코드는 파일 경로(ID)를 기반으로 지식 기반 문서의 콘텐츠를 검색하는 도우미 함수 load_context_chunk_from_data를 정의한 다음 검색 결과를 반복하여 파일 경로, 분류, 문서의 스니펫, 파일에서 로드된 전체 문서 콘텐츠를 포함하여 각 결과에 대한 정보를 인쇄하여 검색 결과에 대한 자세한 표시를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30e5a6a-265b-4fa6-ae37-f5f38fe6296c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- RESULT 1 ----------------\n",
      "\n",
      "FILE PATH: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-cml.txt\n",
      "CLASSIFICATION: public\n",
      "DOCUMENT: Cloudera AI ArchitectureCloudera Docs\n",
      "Cloudera AI Architecture\n",
      "Once a Cloudera AI Workbench is provisioned, you can start using Cloudera AI for your end-to-end Machine Learning workflow. \n",
      "Cloudera AI is a three-tier application that consists of a presentation tier, an application tier\n",
      "and a data tier. \n",
      "Web tier\n",
      "Cloudera AI is a web application that provides a UI that simplifies the action of managing\n",
      "workloads and resources for data scientists. It offers users a convenient way to deploy\n",
      "and scale their analytical pipeline and collaborate with their colleagues in a secure\n",
      "compartmentalized environment. \n",
      "Cloudera AI communicates using HTTPS, Websocket, and gRPC. External communication is limited to\n",
      "HTTP and Websocket for the web UI and APIs. In-cluster service-to-service communication\n",
      "uses gRPC and is encrypted and mutually authenticated using TLS (Transport Layer\n",
      "Security). \n",
      "\n",
      "Application tier\n",
      "The application tier uses an actual set of workloads that users are running. These\n",
      "workloads are isolated in Kubernetes namespaces and run on specially marked compute\n",
      "nodes. Kubernetes/node level auto scaling is used to expand/contract the cluster size\n",
      "based on user demand.\n",
      "User code gets baked into Docker images via a set of Source-to-Image pods (S2I), which\n",
      "includes a managing server, a queue for builds, a registry that serves the images for\n",
      "Docker, a git server, and the builders that actually perform the image building.\n",
      "Traditionally these images used the host machine's docker, but Cloudera AI switched to\n",
      "in-container builds for security and compliance on some platforms. \n",
      "\n",
      "Data tier\n",
      "Cloudera AI uses an internal Postgres database for persisting metadata of user workloads such as\n",
      "Sessions, Jobs, Models and Applications, which runs as a pod and is backed by a\n",
      "persistent volume, using the cloud-provider's block storage offering (for example, EBS\n",
      "for AWS and Premium_LRS for Azure). \n",
      "Cloudera AI uses an NFS server, i.e. a POSIX-compliant file system, for storing users project\n",
      "files, which include user code, libraries installed into a Session, and small data\n",
      "files. For AWS, Cloudera AI creates an Elastic File System (EFS) file system when provisioning\n",
      "the workbench for storing project files. For Azure, users can either provide an NFS\n",
      "volume created in Azure NetApp Files or an external NFS server when provisioning Cloudera AI Workbench for storing project files. This NFS server is supplied to the Cloudera AI Workbenches\n",
      "as Kubernetes Persistent Volumes (PVs). Persistent Volume Claims (PVCs) are set up\n",
      "per-namespace, and each user gets their own namespace - thus each users view of the NFS\n",
      "server is limited to that exposed by the Cloudera Private Cloud. \n",
      "\n",
      "\n",
      "Parent topic: Architecture Overview\n",
      "\n",
      "FULL DOCUMENT (FROM FILE): Cloudera AI ArchitectureCloudera Docs\n",
      "Cloudera AI Architecture\n",
      "Once a Cloudera AI Workbench is provisioned, you can start using Cloudera AI for your end-to-end Machine Learning workflow. \n",
      "Cloudera AI is a three-tier application that consists of a presentation tier, an application tier\n",
      "and a data tier. \n",
      "Web tier\n",
      "Cloudera AI is a web application that provides a UI that simplifies the action of managing\n",
      "workloads and resources for data scientists. It offers users a convenient way to deploy\n",
      "and scale their analytical pipeline and collaborate with their colleagues in a secure\n",
      "compartmentalized environment. \n",
      "Cloudera AI communicates using HTTPS, Websocket, and gRPC. External communication is limited to\n",
      "HTTP and Websocket for the web UI and APIs. In-cluster service-to-service communication\n",
      "uses gRPC and is encrypted and mutually authenticated using TLS (Transport Layer\n",
      "Security). \n",
      "\n",
      "Application tier\n",
      "The application tier uses an actual set of workloads that users are running. These\n",
      "workloads are isolated in Kubernetes namespaces and run on specially marked compute\n",
      "nodes. Kubernetes/node level auto scaling is used to expand/contract the cluster size\n",
      "based on user demand.\n",
      "User code gets baked into Docker images via a set of Source-to-Image pods (S2I), which\n",
      "includes a managing server, a queue for builds, a registry that serves the images for\n",
      "Docker, a git server, and the builders that actually perform the image building.\n",
      "Traditionally these images used the host machine's docker, but Cloudera AI switched to\n",
      "in-container builds for security and compliance on some platforms. \n",
      "\n",
      "Data tier\n",
      "Cloudera AI uses an internal Postgres database for persisting metadata of user workloads such as\n",
      "Sessions, Jobs, Models and Applications, which runs as a pod and is backed by a\n",
      "persistent volume, using the cloud-provider's block storage offering (for example, EBS\n",
      "for AWS and Premium_LRS for Azure). \n",
      "Cloudera AI uses an NFS server, i.e. a POSIX-compliant file system, for storing users project\n",
      "files, which include user code, libraries installed into a Session, and small data\n",
      "files. For AWS, Cloudera AI creates an Elastic File System (EFS) file system when provisioning\n",
      "the workbench for storing project files. For Azure, users can either provide an NFS\n",
      "volume created in Azure NetApp Files or an external NFS server when provisioning Cloudera AI Workbench for storing project files. This NFS server is supplied to the Cloudera AI Workbenches\n",
      "as Kubernetes Persistent Volumes (PVs). Persistent Volume Claims (PVCs) are set up\n",
      "per-namespace, and each user gets their own namespace - thus each users view of the NFS\n",
      "server is limited to that exposed by the Cloudera Private Cloud. \n",
      "\n",
      "\n",
      "Parent topic: Architecture Overview\n",
      "\n",
      "------------- RESULT 2 ----------------\n",
      "\n",
      "FILE PATH: /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-provisioning.txt\n",
      "CLASSIFICATION: public\n",
      "DOCUMENT: ProvisioningCloudera Docs\n",
      "Provisioning\n",
      "Cloudera AI utilizes the Cloudera Control Plane to manage Data Services so you can provision and\n",
      "delete Cloudera AI Workbench. Cloudera Control Plane leverages cloud native capabilities to dynamically\n",
      "access CPU, memory, and GPU resources along with cloud-managed Kubernetes (K8s) to provide the\n",
      "infrastructure. \n",
      "During provisioning the Cloudera AI application is configured to authenticate end\n",
      "users of the service (Cloudera AI) via the Cloudera identity provider, which is chained back to the\n",
      "customers identity provider. As a result, Cloudera AI provisioned instances allow for\n",
      "seamless customer SSO. \n",
      "When you provision a Cloudera AI Workbench, the following happens:\n",
      "\n",
      "Cloudera Control Plane performs the following in the cloud environment: \n",
      "Requests a TLS Certificate and domain name with the cloudera.site domain\n",
      "Identifies the SSO configuration\n",
      "Identifies the SDX configuration for the environment \n",
      "\n",
      " Cloudera Control Plane provisions a managed Kubernetes cluster \n",
      "Cloudera Control Plane installs Cloudera AI into the Kubernetes environment \n",
      "Storage is mounted directly via managed service providers \n",
      "\n",
      "Cloudera AI uses the cloud provider load balancer and networking infrastructure to partition the\n",
      "resources. Cloudera AI also leverages the cloud provider infrastructure to enable the customer to\n",
      "specify autoscaling. \n",
      "Cloudera AI provisions the DNS and the certificate. Cloudera AI renews the certificates for the customer on\n",
      "an ongoing basis. \n",
      "\n",
      "Parent topic: Architecture Overview\n",
      "\n",
      "FULL DOCUMENT (FROM FILE): ProvisioningCloudera Docs\n",
      "Provisioning\n",
      "Cloudera AI utilizes the Cloudera Control Plane to manage Data Services so you can provision and\n",
      "delete Cloudera AI Workbench. Cloudera Control Plane leverages cloud native capabilities to dynamically\n",
      "access CPU, memory, and GPU resources along with cloud-managed Kubernetes (K8s) to provide the\n",
      "infrastructure. \n",
      "During provisioning the Cloudera AI application is configured to authenticate end\n",
      "users of the service (Cloudera AI) via the Cloudera identity provider, which is chained back to the\n",
      "customers identity provider. As a result, Cloudera AI provisioned instances allow for\n",
      "seamless customer SSO. \n",
      "When you provision a Cloudera AI Workbench, the following happens:\n",
      "\n",
      "Cloudera Control Plane performs the following in the cloud environment: \n",
      "Requests a TLS Certificate and domain name with the cloudera.site domain\n",
      "Identifies the SSO configuration\n",
      "Identifies the SDX configuration for the environment \n",
      "\n",
      " Cloudera Control Plane provisions a managed Kubernetes cluster \n",
      "Cloudera Control Plane installs Cloudera AI into the Kubernetes environment \n",
      "Storage is mounted directly via managed service providers \n",
      "\n",
      "Cloudera AI uses the cloud provider load balancer and networking infrastructure to partition the\n",
      "resources. Cloudera AI also leverages the cloud provider infrastructure to enable the customer to\n",
      "specify autoscaling. \n",
      "Cloudera AI provisions the DNS and the certificate. Cloudera AI renews the certificates for the customer on\n",
      "an ongoing basis. \n",
      "\n",
      "Parent topic: Architecture Overview\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper function to return the Knowledge Base doc based on Knowledge Base ID (relative file path)\n",
    "def load_context_chunk_from_data(id_path):\n",
    "    with open(id_path, \"r\") as f: # Open file in read mode\n",
    "        return f.read()\n",
    "    \n",
    "## Clean up output and display full file\n",
    "for i in range(len(results['ids'][0])):\n",
    "    file_path = results['ids'][0][i]\n",
    "    classification = results['metadatas'][0][i]['classification']\n",
    "    document = results['documents'][0][i]\n",
    "    \n",
    "    print(\"------------- RESULT \" + str(i+1) + \" ----------------\\n\")\n",
    "    print(f\"FILE PATH: {file_path}\")\n",
    "    print(f\"CLASSIFICATION: {classification}\")\n",
    "    print(f\"DOCUMENT: {document}\\n\")\n",
    "    print(f\"FULL DOCUMENT (FROM FILE): {load_context_chunk_from_data(file_path)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
